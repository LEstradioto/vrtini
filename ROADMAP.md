# Roadmap — Visual Diff Intelligence

## Vision

Build a best‑in‑class visual regression platform that explains _what changed_ and _why_, not just where pixels differ. The system should classify changes (text, layout, spacing, style, background) and produce a natural‑language report that is useful for humans and AI‑assisted fixes.

## Nice to have

- Do your usual visual regression testing with vrtini.
- Enable AI analysis with structured insights and natural‑language summaries.
- Send the best Prompt generated by our tool to send to your AI pair programmer (Claude or Codex...) to match the required design, using the already design system you have.
- So from Figma, reference Image or URLs, you could generate a PRD to produce a new page or component matching the current design system.

## Now (Near‑Term)

- [ ] Define a first‑class “analysis report” format for each comparison (JSON + human‑readable summary).
- [ ] Add DOM snapshot capture to comparisons (layout boxes + computed styles + text boxes).
- [ ] Add text diff (content and position) and layout drift diff (box movement + size changes).
- [ ] Add a lightweight classification layer: text change, layout shift, spacing change, style change, background change.
- [ ] Ship a report view that shows per‑item insights alongside the visual diff.
- [x] Add a **Detailed Diff Report** modal (similar to AI triage detail) for every comparison item:
  - show all engine outputs (pixelmatch/odiff/ssim/phash/dom),
  - show configured thresholds vs observed values,
  - explain pass/fail/flag decision in plain language.
- [x] Add a **Threshold Timeline/Range UI** in report view:
  - normalized 0-100 rail with checkpoints,
  - markers for configured threshold, measured score, and recommendation zone.
- [x] Add a **Config Explainer** panel:
  - explain each relevant config section and expected impact,
  - highlight likely knobs to tweak for the current mismatch pattern.

## UX Flow Upgrades (Planned)

- [x] Keep row checkboxes always visible in both grid and list views.
- [x] Add sticky/header “Select all rows” checkbox with consistent datatable behavior.
- [x] Ensure test runs always show live progress feedback (same clarity level as compare progress).
- [x] Add footer **Full Run** action: run tests first, then auto-run cross-compare pipeline.
- [ ] Improve the confidence-level dashboard to be more readable/actionable by default.
- [ ] Make UX more opinionated by default (simple happy-path), with Advanced/Details panels for power users.

## Analysis Pipeline (Spec v1)

- Inputs: baseline image, test image, DOM snapshot (layout + styles + text boxes).
- Visual metrics: pixel diff map + perceptual score (LPIPS/DISTS/FLIP to be chosen).
- Text diff: normalized text content comparison + text box movement detection.
- Layout diff: element bounding box deltas (x/y/width/height) and parent‑child spacing deltas.
- Style diff: computed style changes (background, border, color, font metrics).
- Classification rules: map changes to “content”, “layout”, “spacing”, “style”, “background”.
- Output: per‑item findings with severity and a short explanation.
- [ ] Expand multi-engine scoring pipeline with modern signals:
  - LPIPS
  - CLIP-based semantic similarity
  - Apple Vision features (where available)
  - DINOv3-style visual embeddings

## Diff Studio (Front‑End Tool)

- [ ] A UI where users provide a URL or image and compare against another URL or image.
- [ ] One‑click snapshot capture (local dev targets like `localhost` supported).
- [ ] Visual diff + structured analysis + natural‑language report.
- [ ] “Comment” workflow: generate a shareable report link with insights and suggested fixes.
- [ ] AI summary that explains changes in human terms, grounded in deterministic metrics.

## Component Atlas (Design System Extraction)

- [ ] Crawl the app and extract reusable UI components as images.
- [ ] If Storybook exists, capture and index all stories automatically.
- [ ] Cluster and label components into a “component atlas” (storybook‑like catalog).
- [ ] Store reference renders to enable component‑level diffs.
- [ ] When a Figma design is provided, map components to known references for faster implementation guidance.

## AI‑Assisted Fix Suggestions

- [ ] Use analysis report + component atlas to generate targeted suggestions (“card padding increased by 6px”).
- [ ] Provide fix hints as structured diffs (CSS/spacing changes) alongside natural language.
- [ ] Keep pass/fail gating deterministic; AI only summarizes and proposes fixes.

## Later / Stretch

- [ ] Multi‑browser normalization heuristics (font metrics, subpixel rounding, rendering differences).
- [ ] Per‑component tolerance thresholds (e.g., “button label kerning acceptable”).
- [ ] Explainability heatmaps for layout vs style vs content changes.
- [ ] Modularize architecture for cross-product reuse:
  - `core` (jobs/config/io/contracts)
  - `capture` (playwright/docker/browser adapters)
  - `scorer` (pixel/perceptual/semantic engines + aggregation)
  - `vision` (AI triage/reasoning/chunking providers)
  - `tokens` (theme/style/design tokens integration)
  - target reuse across sibling apps (Designmux / Eyeball).

## Research Tracks (Foundation)

- [ ] Graph‑based layout diff (network graph over DOM/layout nodes)
  - Foundation: model each snapshot as a graph (`node = element`, `edges = parent-child + visual adjacency`), then compare graph topology + geometry deltas to detect structural regressions with less pixel noise.
  - Why: catches meaningful layout rewires (moved/removed/re-parented blocks) that pure pixel metrics can miss.
  - Scope v1: start with lightweight graph features (degree, depth, neighborhood overlap, edge-weighted bbox deltas), then evaluate matching strategies (Hungarian / bipartite matching) before any GNN complexity.

- [ ] Project‑level adaptive learning for thresholds and approval prediction
  - Foundation: learn from historical human actions (`approve/reject/flag/revoke`) plus metrics (diff%, SSIM, pHash, DOM findings, scenario, viewport, browser pair).
  - Why: each project has distinct tolerance; a per-project model can reduce manual triage while keeping confidence calibrated.
  - Scope v1: build an online feature store + interpretable model (logistic / gradient boosting) to propose `recommended action + confidence`, with strict human override and audit trail.
