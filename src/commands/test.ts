import type { Command } from 'commander';
import { mkdir, readdir, writeFile } from 'fs/promises';
import { loadConfig } from '../config.js';
import { runScreenshotTasks } from '../docker.js';
import { compareImages } from '../compare.js';
import type { ComparisonResult } from '../types/index.js';
import { isDiff } from '../types/index.js';
import { generateReport } from '../report.js';
import { analyzeWithAI, type AIProvider } from '../ai-analysis.js';
import { calculateConfidence } from '../confidence.js';
import {
  getProjectDirs,
  getReportPath,
  getImageMetadataPath,
  getSnapshotFilename,
} from '../core/paths.js';
import { getErrorMessage } from '../core/errors.js';
import { log } from '../core/logger.js';
import { openInBrowser } from './utils.js';
import {
  buildEnginesConfig,
  buildComparisonMatrix,
  type ComparisonTask,
} from '../core/compare-runner.js';
import { runWithConcurrency } from '../core/async.js';
import {
  buildImageMetadataIndex,
  IMAGE_METADATA_SCHEMA_VERSION,
  type ImageMetadata,
} from '../core/image-metadata.js';
import type { VRTConfig } from '../core/config.js';
import { classifyFindings, classificationToCategory } from '../domain/classification.js';
import type { DomDiffContext } from '../domain/ai-prompt.js';

function buildStatusInfo(result: ComparisonResult): { status: string; info: string } {
  switch (result.reason) {
    case 'match':
      return { status: 'âœ“', info: 'passed' };
    case 'no-baseline':
      return { status: 'â—‹', info: 'new' };
    case 'no-test':
      return { status: '!', info: 'no test image' };
    case 'error':
      return { status: '!', info: result.error };
    case 'diff': {
      const parts: string[] = [`${result.diffPercentage.toFixed(2)}% diff`];
      if (result.ssimScore !== undefined) {
        parts.push(`SSIM: ${(result.ssimScore * 100).toFixed(1)}%`);
      }
      if (result.phash) {
        parts.push(`pHash: ${(result.phash.similarity * 100).toFixed(0)}%`);
      }
      if (result.engineResults) {
        const errored = result.engineResults.filter((e) => e.error);
        if (errored.length > 0) {
          parts.push(`âš  ${errored.map((e) => e.engine).join(', ')} failed`);
        }
      }
      if (result.confidence) {
        parts.push(
          `conf: ${(result.confidence.score * 100).toFixed(0)}% [${result.confidence.verdict}]`
        );
      }
      if (result.aiAnalysis) {
        parts.push(`AI: ${result.aiAnalysis.category}`);
      }
      return { status: 'âœ—', info: parts.join(' | ') };
    }
  }
}

async function listImages(dir: string): Promise<string[]> {
  try {
    const files = await readdir(dir);
    return files.filter((file) => file.endsWith('.png'));
  } catch {
    return [];
  }
}

async function writeImageMetadataFile(
  dir: string,
  metadataIndex: Record<string, ImageMetadata>
): Promise<void> {
  const files = await listImages(dir);
  const images: Record<string, ImageMetadata> = {};

  for (const filename of files) {
    const metadata = metadataIndex[filename];
    if (metadata) {
      images[filename] = metadata;
    }
  }

  const payload = {
    schemaVersion: IMAGE_METADATA_SCHEMA_VERSION,
    generatedAt: new Date().toISOString(),
    images,
  };

  await writeFile(getImageMetadataPath(dir), JSON.stringify(payload, null, 2));
}

async function persistImageMetadata(
  config: VRTConfig,
  scenarios: VRTConfig['scenarios'],
  dirs: { outputDir: string; baselineDir: string; diffDir: string }
): Promise<void> {
  const metadataIndex = buildImageMetadataIndex(config, scenarios);
  await Promise.all([
    writeImageMetadataFile(dirs.outputDir, metadataIndex),
    writeImageMetadataFile(dirs.diffDir, metadataIndex),
    writeImageMetadataFile(dirs.baselineDir, metadataIndex),
  ]);
}

interface AISettings {
  enabled: boolean;
  analyzeAll: boolean;
  manualOnly: boolean;
  threshold: { maxPHashSimilarity: number; maxSSIM: number; minPixelDiff: number };
  config: VRTConfig['ai'];
}

function resolveAISettings(
  options: { ai?: boolean; aiAll?: boolean },
  config: VRTConfig
): AISettings {
  const enabled =
    options.ai === true || options.aiAll === true
      ? true
      : options.ai === false
        ? false
        : (config.ai?.enabled ?? false);

  return {
    enabled,
    analyzeAll: options.aiAll === true,
    manualOnly: config.ai?.manualOnly ?? false,
    threshold: config.ai?.analyzeThreshold ?? {
      maxPHashSimilarity: 0.95,
      maxSSIM: 0.98,
      minPixelDiff: 0.1,
    },
    config: config.ai,
  };
}

async function captureScreenshots(config: VRTConfig, scenarioFilter?: string[]): Promise<void> {
  log.info('\nðŸ“¸ Capturing screenshots...\n');
  const results = await runScreenshotTasks({ config, scenarios: scenarioFilter });

  const failed = results.filter((r) => !r.success);
  if (failed.length > 0) {
    log.error('\nSome screenshots failed:');
    failed.forEach((f) => {
      log.error(`  - ${f.task.scenario.name}: ${f.error}`);
    });
  }
}

async function compareTask(
  task: ComparisonTask,
  config: VRTConfig,
  quickMode: boolean,
  ai: AISettings
): Promise<ComparisonResult> {
  const { scenario, browser, viewport, testPath, baselinePath, diffPath } = task;
  const enginesConfig = buildEnginesConfig(quickMode, config.engines);

  // Derive snapshot paths from image paths
  const snapshotFilename = getSnapshotFilename(task.filename);
  const baselineSnapshotPath = config.domSnapshot?.enabled
    ? baselinePath.replace(task.filename, snapshotFilename)
    : undefined;
  const testSnapshotPath = config.domSnapshot?.enabled
    ? testPath.replace(task.filename, snapshotFilename)
    : undefined;

  let result: ComparisonResult = await compareImages(baselinePath, testPath, diffPath, {
    threshold: config.threshold,
    diffColor: config.diffColor,
    computePHash: !quickMode,
    engines: enginesConfig,
    antialiasing: config.engines?.pixelmatch?.antialiasing,
    keepDiffOnMatch: config.keepDiffOnMatch,
    maxDiffPercentage:
      scenario.diffThreshold?.maxDiffPercentage ?? config.diffThreshold?.maxDiffPercentage,
    maxDiffPixels: scenario.diffThreshold?.maxDiffPixels ?? config.diffThreshold?.maxDiffPixels,
    baselineSnapshot: baselineSnapshotPath,
    testSnapshot: testSnapshotPath,
  });

  if (isDiff(result)) {
    result = await enrichDiffResult(result, task, ai);
  }

  const { status, info } = buildStatusInfo(result);
  log.info(`  ${status} ${scenario.name} / ${browser} / ${viewport.name}: ${info}`);
  return result;
}

async function enrichDiffResult(
  result: ComparisonResult & { reason: 'diff' },
  task: ComparisonTask,
  ai: AISettings
): Promise<ComparisonResult> {
  let enriched = result;
  const { scenario, browser, viewport } = task;

  // Compute DOM classification if DOM diff is present
  let domDiffContext: DomDiffContext | undefined;
  let domCategory: ReturnType<typeof classificationToCategory> | undefined;
  if (result.domDiff) {
    const classification = classifyFindings(result.domDiff);
    domCategory = classificationToCategory(classification);
    const topFindings = result.domDiff.findings.slice(0, 5).map((f) => f.description);
    domDiffContext = {
      findingCounts: result.domDiff.summary,
      topFindings,
    };
  }

  const needsAI =
    ai.enabled &&
    (ai.analyzeAll ||
      (!ai.manualOnly &&
        (result.phash?.similarity ?? 0) < ai.threshold.maxPHashSimilarity &&
        (result.ssimScore ?? 0) < ai.threshold.maxSSIM &&
        result.diffPercentage >= ai.threshold.minPixelDiff));

  if (needsAI) {
    try {
      log.info(`  ðŸ¤– Analyzing ${scenario.name} / ${browser} / ${viewport.name}...`);
      const aiResult = await analyzeWithAI(task.baselinePath, task.testPath, result.diffPath, {
        provider: (ai.config?.provider ?? 'anthropic') as AIProvider,
        apiKey: ai.config?.apiKey,
        authToken: ai.config?.authToken,
        model: ai.config?.model,
        baseUrl: ai.config?.baseUrl,
        visionCompare: ai.config?.visionCompare,
        scenarioName: scenario.name,
        url: scenario.url,
        pixelDiff: result.pixelDiff,
        diffPercentage: result.diffPercentage,
        ssimScore: result.ssimScore,
        domDiff: domDiffContext,
      });
      enriched = { ...enriched, aiAnalysis: aiResult };
    } catch (err) {
      log.warn(`  âš  AI analysis failed: ${getErrorMessage(err)}`);
    }
  }

  const confidence = calculateConfidence({
    ssimScore: enriched.ssimScore,
    phashSimilarity: enriched.phash?.similarity,
    pixelDiffPercent: enriched.diffPercentage,
    aiAnalysis: isDiff(enriched) ? enriched.aiAnalysis : undefined,
    domCategory,
    domSummary: enriched.domDiff?.summary,
  });
  return { ...enriched, confidence };
}

function collectEngineStatus(comparisons: ComparisonResult[]): {
  ran: Set<string>;
  errored: Map<string, number>;
} {
  const ran = new Set<string>();
  const errored = new Map<string, number>();

  for (const r of comparisons) {
    if (!isDiff(r) || !r.engineResults) continue;
    for (const er of r.engineResults) {
      ran.add(er.engine);
      if (er.error) {
        errored.set(er.engine, (errored.get(er.engine) ?? 0) + 1);
      }
    }
  }
  return { ran, errored };
}

function printSummary(comparisons: ComparisonResult[], quickMode: boolean): void {
  const passed = comparisons.filter((r) => r.match).length;
  const failed = comparisons.filter((r) => r.reason === 'diff').length;
  const newCount = comparisons.filter((r) => r.reason === 'no-baseline').length;
  const aiAnalyzed = comparisons.filter((r) => isDiff(r) && r.aiAnalysis).length;

  log.info('\nðŸ“‹ Summary:');
  log.info(`  âœ“ ${passed} passed`);
  log.info(`  âœ— ${failed} failed`);
  log.info(`  â—‹ ${newCount} new`);

  if (aiAnalyzed > 0) {
    log.info(`  ðŸ¤– ${aiAnalyzed} AI analyzed`);
  }

  // Engine status
  const allEngines = ['pixelmatch', 'odiff', 'ssim', 'phash'] as const;
  const { ran, errored } = collectEngineStatus(comparisons);

  if (failed > 0 || ran.size > 0) {
    const engineStatus = allEngines.map((e) => {
      if (quickMode && e !== 'pixelmatch') return `${e}: skipped`;
      if (errored.has(e)) return `${e}: âš  ${errored.get(e)} errors`;
      if (ran.has(e)) return `${e}: âœ“`;
      return `${e}: disabled`;
    });
    log.info(`\n  ðŸ”§ Engines: ${engineStatus.join(' | ')}`);
  }
}

export function registerTestCommand(program: Command): void {
  program
    .command('test')
    .description('Run visual regression tests')
    .option('-c, --config <path>', 'Path to config file')
    .option('-s, --scenario <name...>', 'Run specific scenarios')
    .option('--skip-screenshots', 'Skip screenshot capture, only compare')
    .option('-q, --quick', 'Quick mode: fast comparison (pixelmatch only, skips SSIM/pHash)')
    .option('--ai', 'Enable AI analysis (overrides config)')
    .option('--no-ai', 'Disable AI analysis (overrides config)')
    .option('--ai-all', 'Analyze ALL diffs with AI (ignore thresholds)')
    .option('-o, --open', 'Open report in browser after tests')
    .action(async (options) => {
      try {
        const config = await loadConfig(options.config);
        const cwd = process.cwd();
        const { outputDir, baselineDir, diffDir } = getProjectDirs(cwd, config);

        await mkdir(outputDir, { recursive: true });
        await mkdir(baselineDir, { recursive: true });
        await mkdir(diffDir, { recursive: true });

        if (!options.skipScreenshots) {
          await captureScreenshots(config, options.scenario);
        }

        log.info('\nðŸ” Comparing screenshots...\n');

        const scenarios = options.scenario
          ? config.scenarios.filter((s: { name: string }) => options.scenario.includes(s.name))
          : config.scenarios;

        const quickMode = options.quick || config.quickMode;
        if (quickMode) {
          log.warn(
            'âš  Quick mode: only pixelmatch enabled (ssim, phash, odiff skipped). Confidence scores are limited.'
          );
        }
        const ai = resolveAISettings(options, config);

        const comparisonTasks = buildComparisonMatrix(
          outputDir,
          baselineDir,
          diffDir,
          scenarios,
          config
        );
        const comparisons = await runWithConcurrency(
          comparisonTasks,
          config.concurrency ?? 5,
          (task) => compareTask(task, config, quickMode, ai)
        );

        log.info('\nðŸ“Š Generating report...\n');

        await persistImageMetadata(config, scenarios, { outputDir, baselineDir, diffDir });

        const reportPath = getReportPath(cwd, config);
        await generateReport(
          {
            title: 'vrtini Report',
            timestamp: new Date().toISOString(),
            results: comparisons,
            baselineDir,
            outputDir,
            aiEnabled: ai.enabled,
          },
          { outputPath: reportPath, embedImages: config.report?.embedImages }
        );

        log.info(`Report saved: ${reportPath}`);

        if (options.open) {
          openInBrowser(reportPath);
        }

        printSummary(comparisons, quickMode);

        if (comparisons.some((r) => r.reason === 'diff')) {
          process.exit(1);
        }
      } catch (err) {
        log.error('Error:', getErrorMessage(err));
        process.exit(1);
      }
    });
}
